<html>
    <head>
        <link href='https://fonts.googleapis.com/css?family=Roboto' rel='stylesheet'>
        <script src="https://code.jquery.com/jquery-3.6.0.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" crossorigin="anonymous">
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" integrity="sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.9.2/umd/popper.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    </head>
    <style>
        body {
            font-family: 'Roboto';
        }
        h2 {
            padding: 10px 0;
            font-weight: bold;
        }
    </style>
    <body>
        <div class="container">
            <h1 class="py-2 mt-5 border-bottom font-weight-bold">Adventures in Grounding to Object Positions</h1>
            <a class="text-muted" href="https://dylanebert.com">Dylan Ebert</a>
            <div class="mt-3" id="overview">
                <p>This document gives an overview of my recent attempts to make use of spatial data,
                    or the positions of objects over time, to represent verbs.
                    I'll talk about the motivation for this, the models I've tried, and the results so far.
                </p>
            </div>
            <div id="motivation">
                <h2>Why spatial data?</h2>
                <h4>Text-based NLP</h4>
                <p>Most recent progress in NLP takes advantage of very large-scale language datasets and models to solve text-based tasks
                    like question-answering. Take a look at the BoolQ task below.</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="qbool.PNG" class="img-fluid" style="max-width: 512px;">
                        <figcaption class="figure-caption">BoolQ task from SuperGLUE, a common NLP benchmark.</figcaption>
                    </figure>
                </div>
                <p>The input is text, and the output is text. This is the common setup for a question-answering task.
                    <a href="https://arxiv.org/pdf/2103.08493.pdf">Prompt-based training</a> sets up a variety of tasks
                    like a question-answering tasks, and achieves human performance on many of them in a unified framework.
                    So, is language solved? No, not yet. So, what's missing? I, among many others, would argue that grounding is a necessary
                    component of language understanding.</p>
                <h4>Grounded NLP</h4>
                <p>Grounding is the connection of language to the non-linguistic world. What does recent work in grounded NLP look like? 
                    Some years ago, grounding meant image captioning, i.e. is below a picture of a zebra?</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Plains_Zebra_Equus_quagga.jpg/399px-Plains_Zebra_Equus_quagga.jpg" class="img-fluid">
                        <figcaption class="figure-caption">Zebra?</figcaption>
                    </figure>
                </div>
                <p>More recent work has <a href="https://arxiv.org/pdf/2102.02779.pdf">unified vision and language models</a> to learn representations that incorporate both images and text.
                    This still falls short of human performance on tasks such as visual question-answering, but moves the needle toward it.
                    So, with bigger models, more data, and better performance, will language be solved? I don't know. It's more difficult to support an answer to that than to whether grounding is necessary.
                    However, it does raise another interesting question - what else can we do?</p>
                <h4>Embodied NLP</h4>
                <p>So far, I've only talked about tasks that produce some text output. But what about tasks that require doing something in the world?
                    One example is <a href="https://askforalfred.com/">ALFRED</a>, a benchmark for mapping natural language instructions to sequences of actions.
                    In ALFRED, a virtual agent takes egocentric video and text instructions as input, and must execute <i>actions</i> to carry out the instructions.</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="alfreddiagram.png" class="img-fluid">
                        <figcaption class="figure-caption">Example instruction sequence from ALFRED</figcaption>
                    </figure>
                </div>
                <p>This task involves the complex behaviors of objects, which makes for a much more challenging task. This is reflected in the current state-of-the-art performance,
                    which is <a href="https://leaderboard.allenai.org/alfred/submissions/public">under 10% for unseen sequences</a>.
                    Why is the performance so low? What makes this task so much more challenging than text-based tasks? These are the questions I'm most interested in.</p>
                <h4>Broader Perspectives: Abstraction</h4>
                <p>Taking a look at artificial intelligence as a whole, where do text-based, grounded, and embodied NLP fit? 
                    I'll borrow from <a href="https://www.sciencedirect.com/science/article/pii/S2352154618302080">On the Necessity of Abstraction</a>,
                    which highlights the importance of <i>abstraction</i>, or mapping a complex state space to a more simple state space that is computationally tractable to carry out a given task.
                    For example, imagine a game of chess. At the lowest layer of abstraction, we have raw sensory information - visual, auditory, and tactile. At a high level of abstraction,
                    we have an 8x8 grid of chess pieces. But when people think of chess, and when building AI to solve chess, it's not in terms of matrices of RGB values, 
                    but rather the 8x8 grid. Although, sensory input is necessary for actually playing chess, in order to recognize pieces, situate oneself in the environment to
                    reach the pieces, and engage muscles to execute a move.</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="obj-recog.png" class="img-fluid p-3" style="max-width: 512px;">
                        <figcaption class="figure-caption">Abstraction tree for object recognition</figcaption>
                    </figure>
                </div>
                <p>Object recognition, a classic computer vision problem, is a clear example of abstraction. The inputs are images composed of a matrices of RGB values, and the output is object labels.
                    A caveat of this is that the object labels themselves correspond to language, which is entangled with many different levels of abstraction. However, these labels represent
                    objects, or persitent entities that are expected to maintain shape and move continuously. This is an abstraction on raw visual input that is shown to exist from infancy, long before language.</p>
                <p>So, what might language look like in an abstraction tree?</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="grounded.png" class="img-fluid p-3" style="max-width: 512px;">
                        <figcaption class="figure-caption">Abstraction tree for language</figcaption>
                    </figure>
                </div>
                <p>Above is a simplified example of how language may fit into an abstraction tree, in terms of problems in AI. Language, in the top right, is composed only of nouns and verbs, for simplicity.
                    In most modern language models, this is a self-contained subtree, where language is based only on language. In the case of grounded language models, additional information comes directly from vision,
                    at the bottom of the abstraction tree. Elsewhere in the tree are fields like speech recognition, which abstracts from audio to words, and simulation, which constrains the objects to some common behavior,
                    labeled here as "physics". These don't play any direct role in state-of-the-art language models. So how are these models successful?</p>
                <h4>Fitting things together</h4>
                <p>Though modern language models don't explicitly use information from lower-level inputs like physics or audio, probing experiments have shown that language models contain information
                    that is applicable to a wide variety of tasks. This may be explained by the idea that large amounts of information about the world and our perception at various levels of abstraction are baked into
                    our use of language, and can be learned through distributional language models. However, how and the extent to which this information is learned is unclear, and, as previously stated, currently don't succeed
                    at complex situated tasks like instruction following.</p>
                <p>The purpose of this research is to hone in on a sparsely-researched part of the abstraction tree: verbs from physics, as shown in the figure above. 
                    Specifically, I'm interested in low-level verbs like <i>pick</i>, <i>put</i>, and <i>throw</i>. Most prior work in verb learning uses vision as input. Learning verbs from
                    video is a very challenging problem in AI. On the other hand, humans already have some mental model of object motion from infancy, and acquire language later. Given this information, the question
                    this research asks is:</p>
                <blockquote class="blockquote px-5">Using the spatial data of objects as input, what representations can we learn? How well do these representations align with verbs?</blockquote>
            </div>
            <div id="spatial" class="mt-5">
                <h2>Spatial data: In practice</h2>
                <h4>Terms and Definitions</h4>
                <p>Though we know that humans are able to represent objects and their physical behavior from infancy, what does this representation actually look like? Unfortunately, we don't know.
                    In my case, I choose to go with common euclidean space, where each object position is represented as a 3d vector with components $\{x, y, z\}$, and object rotation
                    is represented as a 4d quaternion with components $\{x, y, z, w\}$.</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="rollpitchyaw.png" class="img-fluid p-3" style="max-width: 512px;">
                        <figcaption class="figure-caption">Roll (rotation about the z-axis), Pitch (x-axis), and Yaw (y-axis)</figcaption>
                    </figure>
                </div>
                <p>I'll be referencing rotation in terms of <i>roll, pitch, and yaw</i> interchangeably with axes. Axes aren't universally consistent, but in this work, I use the <i>y-up</i> configuration
                    consistent with Unity, where <b>roll</b> is around the z-axis, <b>pitch</b> is around the x-axis, and <b>yaw</b> is around the y-axis.</p>
                <div class="row my-4">
                    <div class="col-sm-6">
                        <video autoplay muted loop style="max-width: 100%;">
                            <source type="video/mp4" src="video.mp4">
                        </video> 
                    </div>
                    <div class="col-sm-6">
                        <canvas id="chart"></canvas>
                    </div>
                </div>
                <p>These position values may also be referred to as <b>absolute position</b>, kept relative to an arbitrary static world origin. These values can encounter problems in
                    scenes with different origins or orientations. For example, if a linear regression were fit to a collection of scenes where the floor is at y-position $1.5$, and tested on a scene where the floor is at y-position
                    $-2$, it would likely fail.</p>
                <p>One way to address this is to use <b>relative position</b>, or position from the perspective of the viewer. In other words, no matter which way a viewer is
                    looking, an object moving directly away from the viewer will be moving along the <i>relative</i> z-axis. A caveat of this is that the axis of gravity is unstable,
                    since the direction of "up" changes with respect to the roll and pitch of the viewer.
                    Another option is what I'll refer to is <b>look position</b>, or position rotated about the y-axis with respect to the viewer. This way, $\{x, z\}$ rotate with
                    the direction the viewer is looking, but $y$ remains consistent with the global y-axis, or the axis of gravity. 
                    Both of these are sensitive to the motion of the viewer. That is, all objects in a scene moving upward look identical
                    to the viewer moving downward.</p>
                <p>Finally, I'll be referencing <b>velocity</b>, or the change in position over time. Velocity is robust to different scene origins and orientations, but is unable to capture
                    spatial relations, e.g. <i>the ball in on top of the table</i>.</p>
                <h4>Special Considerations</h4>
                <p>There are some things to consider when using euclidean spatial data, as opposed to words or images. These are not well-supported claims, just what I've noticed from my experiments.</p>
                <ul class="list-group">
                    <li class="list-group-item"><b>1. Transformers won't work.</b> The input is continuous and unbounded, so models that rely on discrete tokenized input, such as attention mechanisms, won't work.</li>
                    <li class="list-group-item"><b>2. There are no good labels.</b> Since the goal of this project is to learn representations of object motion that <i>may or may not align with verbs</i>,
                        there's no way to dictate exactly how a good representation should look. A good representation is one that is useful to carry out tasks higher in the abstraction tree, not necessarily
                        one that aligns perfectly with low-level English verbs.</li>
                    <li class="list-group-item"><b>3. It's difficult to normalize.</b> Euclidean space is infinite, so an apple may theoretically be moving around 7 billions meters away or 1 meter away.
                        This is in contrast to image data, where RGB values are bounded to $[0, 1]$. Typically, people just constrain the space and it isn't that big of a deal. But in trying to build
                        an intelligent agent that can adapt to various environments, it becomes a bigger problem.</li>
                    <li class="list-group-item"><b>4. Real spatial data is noisy.</b> Imagine someone picking up an apple. There may be a clear, consistent trajectory this action follows.
                        Now, compare this to a person quickly picking up an apple off a table in half a second. Compare that to someone picking up an apple off the ground in ten seconds, while shaking.
                        In reality, when looking at natural data of people <i>doing stuff</i>, good examples of "pick up" may have extremely different trajectories.</li>
                </ul>
            </div>
            <div id="models" class="mt-4">
                <h2>Models</h2>
                <h4>Preamble</h4>
                <p>Now that I've gone over spatial data and why I'm using it, I'll talk about the things I've actually tried so far. Each approach I've tried doesn't correspond 1:1 to a model, but to a many-to-many
                    collection of models, which makes it difficult to explain each approach in isolation. Therefore, I'll first go over the existing models I've used, before going over how the
                    pieces fit together in the <a href="#approaches">Approaches</a> section.</p>
                <p>However, before going into the models, I'll explain some technical parameters of the problem. I'm using the
                    <a href="https://lunar.cs.brown.edu/nbc/">New Brown Corpus</a> dataset, which consists of participants narrating while performing everyday tasks in virtual kitchens.</p>
                <div class="text-center my-4">
                    <video width="960" height="540" controls>
                        <source src="https://plunarlabcit.services.brown.edu/videos/1_1a_task1.mp4" type="video/mp4">
                    </video>
                </div>
                <p>The dataset is broken up into 108 <i>recording sessions</i> that are around 5 minutes long each. Each has corresponding narration, video, and spatial data. The spatial data includes
                    the <b>position</b> and <b>rotation</b> of every object (including the head and hands), every frame. There are 6 kitchens, 3 per participant. In all experiments, the participants are
                    split up 14-2-2 train-dev-test, so each kitchen is seen at least once at train time.</p>
                <p>The goal of each approach is to learn "actions", or cohesive, interesting patterns of motion that may align with verbs, or be useful for downstream verb-learning.
                    In general, learning representations of actions can be broken down into segmentation and clustering. <b>Segmentation</b> is cutting the sessions
                    into chunks of time, where each chunk of time corresponds to an action. <b>Clustering</b> is grouping similar chunks of time together, such that each cluster is representative of some
                    action. Finally, <b>Transformation</b> is manipulating the data to make it easier to do segmentation and clustering. Each model described serves one or more of these three purposes.</p>
                <!--TODO add timeline figure that shows segmentation pointing to graph that shows clustering-->
                <h4>Feature-engineering</h4>
                <p><i>Feature-engineering</i> is an umbrella term referring to any <b>transformations</b> I make that don't correspond to a model.
                    In practice, this typically consists of preprocessing steps like scaling or trimming outliers,
                    but may also involve more complex transformations like taking the magnitude velocity of the most moving object,
                    representing a trajectory in terms of its peak and trough, etc.</p>
                <h4>K-means</h4>
                <p>I use K-means for unsupervised <b>clustering</b>, which clusters data according to distance to centroids.
                    The number of clusters must be specified. Likewise for time-series data, I use 
                    <a href="https://tslearn.readthedocs.io/en/stable/gen_modules/clustering/tslearn.clustering.TimeSeriesKMeans.html">
                    Time Series K-Means</a>. With a
                    <a href="https://en.wikipedia.org/wiki/Dynamic_time_warping">Dynamic Time Warping</a> 
                    distance function, this algorithm allows time series data of different lengths to be clustered.</p>
                <p>Related to K-means, I also use <a href="https://www.is.mpg.de/uploads_file/attachment/attachment/442/1000-v2.pdf">Hierarchical Dynamic Clustering</a>,
                    a clustering algorithm for which the number of clusters is unspecified. Instead, a parameter is set that influences the number of clusters. This is
                    advantageous in cases where it's difficult to know a suitable number of clusters.</p>
                <h4>Energy-based motion pooling</h4>
                <p><a href="https://www.is.mpg.de/uploads_file/attachment/attachment/442/1000-v2.pdf">Energy-based motion pooling</a> 
                    is a <b>segmentation</b> model based on identifying energy or motion <i>peaks</i>, and considering windows around them to
                    be motion <i>segments</i>. The way this is done is as follows:</p>
                <ol>
                    <li>Discretize the position data into many small clusters, effectively binning the data.</li>
                    <li>Using a sliding window across the data, compute the <i>energy</i> of each window, or number of transitions between clusters.</li>
                    <li>Apply a gaussian smoothing filter to the energy curve.</li>
                    <li>Identify peaks in the energy curve.</li>
                    <li>Designate actions as windows around peaks based on peak prominence.</li>
                </ol>
                <h4>Hidden semi-markov model</h4>
                <p>The <a href="https://en.wikipedia.org/wiki/Hidden_semi-Markov_model">hidden semi-markov model</a> is a bayesian model that simultaneously performs <b>segmentation</b> and <b>clustering</b>.
                    Like a hidden markov model, it consists of states and transitions, and the ability to compute the maximum
                    likelihood state sequence given some data. However, unlike a hidden markov model,
                    the hidden <b>semi-</b>markov model also considers the number of timesteps or expected <i>length</i> in each state.
                    By taking the maximum-likelihood state sequence of a data sequence given a hidden semi-markov model, we
                    end up with a segmented, labeled sequence.</p>
                <h4>Principle Component Analysis</h4>
                <p>Principle Component Analysis or PCA is a common algorithm that reduces the dimensionality of data while trying
                    to retain as much information as possible. It is a popular <b>transformation</b> of data.</p>
                <h4>Autoencoder</h4>
                <p>Like PCA, an autoencoder or AE <b>transforms</b> data by encoding it to a smaller representation.
                    It does so using an encoder-decoder neural network architecture, where the encoder network encodes
                    the data to a reduced representation, and the decoder network attempts to reconstruct the original input
                    from the reduced representation.</p>
                <p>I also use a variational autoencoder or VAE, a probabilistic variation of the autoencoder that produces a smooth
                    encoded manifold. This means that, if unseen points are sampled from the encoding space, it should
                    produce plausible outputs.</p>
                <h4>LSTM</h4>
                <p>Finally, I <b>transform</b> data using an LSTM, a form of recurrent neural network that models sequential data
                    by processing timesteps in terms of previous timesteps. LSTMs are very popular modern baseline when working
                    with sequential data like sentences or videos.</p>
            </div>
            <div id="approaches">
                <h2>Approaches</h2>
                <h4>HSMM based</h4>
                <p>Most of my time spent working with spatial data, I've done so with an HSMM based pipeline.
                    This pipeline can be summarized as follows:</p>
                <ol>
                    <li>Some <b>transformations</b> to the data, such as <b>feature-engineering</b>, <b>autoencoders</b> or
                    <b>LSTMs</b>.</li>
                    <li>An <b>HSMM</b> for simultaenous <b>segmentation</b> and <b>clustering</b>.</li>
                </ol>
                <p>Throughout these attempts, representing months of research, the question was:</p>
                <blockquote class="blockquote px-5">How can I transform the data so that the HSMM is able to produce the best
                    possible segments and clusters?</blockquote>
                <p>For the HSMM itself, I stuck to a Gaussian observation model. That is, each state can only produce
                    values according to some mean and variation. This means that a state is unable to represent dynamic data,
                    i.e. a single state can't represent the $\{x, y, z\}$ position of an object moving from one place to another.
                    Therefore, the goal is to represent this motion in terms of values that don't change (except when trying to
                    represent a different motion), before feeding it to the hsmm.</p>
                <!--
                    HSMM
                    ---
                    - Direct inputs
                    - LSTM
                    - Autoencoder

                -->
                <h6>Direct Inputs</h6>
                <p>The simplest way to represent motion as a single value is to take the derivative. The derivative of position is
                    velocity.
                </p>
                <h4>K-means based</h4>
                <p>More recently, I decided to try something that doesn't have an HSMM at the end, and opted instead for a simpler approach - K-means.</p>           
            </div>
        </div>
        <footer class="py-5 mt-5 border-top"></footer>
    </body>
    <script>
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });

        $.get('https://raw.githubusercontent.com/dylanebert/spatial_blog/master/data.json', function(res) {
            var data = $.parseJSON(res);
            console.log(data);
            var ctx = $('#chart');
            var chart = new Chart(ctx, {
                type: 'line',
                data: data
            })
        })
    </script>
</html>