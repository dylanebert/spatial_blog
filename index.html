<html>
    <head>
        <link href='https://fonts.googleapis.com/css?family=Roboto' rel='stylesheet'>
        <script src="https://code.jquery.com/jquery-3.6.0.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" crossorigin="anonymous">
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" integrity="sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.9.2/umd/popper.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    </head>
    <style>
        body {
            font-family: 'Roboto';
        }
        h2 {
            padding: 10px 0;
            font-weight: bold;
        }
    </style>
    <body>
        <div class="container">
            <h1 class="py-2 mt-5 border-bottom font-weight-bold">Adventures in Grounding to Object Positions</h1>
            <a class="text-muted" href="https://dylanebert.com">Dylan Ebert</a>
            <div class="mt-3" id="overview">
                <p>This document gives an overview of my recent attempts to make use of spatial data,
                    or the positions of objects over time, to represent verbs.
                    I'll talk about the motivation for this, the models I've tried, and the results so far.
                </p>
            </div>
            <div id="motivation">
                <h2>Why spatial data?</h2>
                <h4>Text-based NLP</h4>
                <p>Most recent progress in NLP takes advantage of very large-scale language datasets and models to solve text-based tasks
                    like question-answering. Take a look at the BoolQ task below.</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="qbool.PNG" class="img-fluid" style="max-width: 512px;">
                        <figcaption class="figure-caption">BoolQ task from SuperGLUE, a common NLP benchmark.</figcaption>
                    </figure>
                </div>
                <p>The input is text, and the output is text. This is the common setup for a question-answering task.
                    <a href="https://arxiv.org/pdf/2103.08493.pdf">Prompt-based training</a> sets up a variety of tasks
                    like a question-answering tasks, and achieves human performance on many of them in a unified framework.
                    So, is language solved? No, not yet. So, what's missing? I, among many others, would argue that grounding is a necessary
                    component of language understanding.</p>
                <h4>Grounded NLP</h4>
                <p>Grounding is the connection of language to the non-linguistic world. What does recent work in grounded NLP look like? 
                    Some years ago, grounding meant image captioning, i.e. is below a picture of a zebra?</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Plains_Zebra_Equus_quagga.jpg/399px-Plains_Zebra_Equus_quagga.jpg" class="img-fluid">
                        <figcaption class="figure-caption">Zebra?</figcaption>
                    </figure>
                </div>
                <p>More recent work has <a href="https://arxiv.org/pdf/2102.02779.pdf">unified vision and language models</a> to learn representations that incorporate both images and text.
                    This still falls short of human performance on tasks such as visual question-answering, but moves the needle toward it.
                    So, with bigger models, more data, and better performance, will language be solved? I don't know. It's more difficult to support an answer to that than to whether grounding is necessary.
                    However, it does raise another interesting question - what else can we do?</p>
                <h4>Embodied NLP</h4>
                <p>So far, I've only talked about tasks that produce some text output. But what about tasks that require doing something in the world?
                    One example is <a href="https://askforalfred.com/">ALFRED</a>, a benchmark for mapping natural language instructions to sequences of actions.
                    In ALFRED, a virtual agent takes egocentric video and text instructions as input, and must execute <i>actions</i> to carry out the instructions.</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="alfreddiagram.png" class="img-fluid">
                        <figcaption class="figure-caption">Example instruction sequence from ALFRED</figcaption>
                    </figure>
                </div>
                <p>This task involves the complex behaviors of objects, which makes for a much more challenging task. This is reflected in the current state-of-the-art performance,
                    which is <a href="https://leaderboard.allenai.org/alfred/submissions/public">under 10% for unseen sequences</a>.
                    Why is the performance so low? What makes this task so much more challenging than text-based tasks? These are the questions I'm most interested in.</p>
                <h4>Broader Perspectives: Abstraction</h4>
                <p>Taking a look at artificial intelligence as a whole, where do text-based, grounded, and embodied NLP fit? 
                    I'll borrow from <a href="https://www.sciencedirect.com/science/article/pii/S2352154618302080">On the Necessity of Abstraction</a>,
                    which highlights the importance of <i>abstraction</i>, or mapping a complex state space to a more simple state space that is computationally tractable to carry out a given task.
                    For example, imagine a game of chess. At the lowest layer of abstraction, we have raw sensory information - visual, auditory, and tactile. At a high level of abstraction,
                    we have an 8x8 grid of chess pieces. But when people think of chess, and when building AI to solve chess, it's not in terms of matrices of RGB values, 
                    but rather the 8x8 grid. Although, sensory input is necessary for actually playing chess, in order to recognize pieces, situate oneself in the environment to
                    reach the pieces, and engage muscles to execute a move.</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="obj-recog.png" class="img-fluid p-3" style="max-width: 512px;">
                        <figcaption class="figure-caption">Abstraction tree for object recognition</figcaption>
                    </figure>
                </div>
                <p>Object recognition, a classic computer vision problem, is a clear example of abstraction. The inputs are images composed of a matrices of RGB values, and the output is object labels.
                    A caveat of this is that the object labels themselves correspond to language, which is entangled with many different levels of abstraction. However, these labels represent
                    objects, or persitent entities that are expected to maintain shape and move continuously. This is an abstraction on raw visual input that is shown to exist from infancy, long before language.</p>
                <p>So, what might language look like in an abstraction tree?</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="grounded.png" class="img-fluid p-3" style="max-width: 512px;">
                        <figcaption class="figure-caption">Abstraction tree for language</figcaption>
                    </figure>
                </div>
                <p>Above is a simplified example of how language may fit into an abstraction tree, in terms of problems in AI. Language, in the top right, is composed only of nouns and verbs, for simplicity.
                    In most modern language models, this is a self-contained subtree, where language is based only on language. In the case of grounded language models, additional information comes directly from vision,
                    at the bottom of the abstraction tree. Elsewhere in the tree are fields like speech recognition, which abstracts from audio to words, and simulation, which constrains the objects to some common behavior,
                    labeled here as "physics". These don't play any direct role in state-of-the-art language models. So how are these models successful?</p>
                <h4>Fitting things together</h4>
                <p>Though modern language models don't explicitly use information from lower-level inputs like physics or audio, probing experiments have shown that language models contain information
                    that is applicable to a wide variety of tasks. This may be explained by the idea that large amounts of information about the world and our perception at various levels of abstraction are baked into
                    our use of language, and can be learned through distributional language models. However, how and the extent to which this information is learned is unclear, and, as previously stated, currently don't succeed
                    at complex situated tasks like instruction following.</p>
                <p>The purpose of this research is to hone in on a sparsely-researched part of the abstraction tree: verbs from physics, as shown in the figure above. 
                    Specifically, I'm interested in low-level verbs like <i>pick</i>, <i>put</i>, and <i>throw</i>. Most prior work in verb learning uses vision as input. Learning verbs from
                    video is a very challenging problem in AI. On the other hand, humans already have some mental model of object motion from infancy, and acquire language later. Given this information, the question
                    this research asks is:</p>
                <blockquote class="blockquote px-5">Using the spatial data of objects as input, what representations can we learn? How well do these representations align with verbs?</blockquote>
            </div>
            <div id="spatial" class="mt-5">
                <h2>Spatial data: In practice</h2>
                <h4>Terms and Definitions</h4>
                <p>Though we know that humans are able to represent objects and their physical behavior from infancy, what does this representation actually look like? Unfortunately, we don't know.
                    In my case, I choose to go with common euclidean space, where each object position is represented as a 3d vector with components $\{x, y, z\}$, and object rotation
                    is represented as a 4d quaternion with components $\{x, y, z, w\}$.</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="rollpitchyaw.png" class="img-fluid p-3" style="max-width: 512px;">
                        <figcaption class="figure-caption">Roll (rotation about the z-axis), Pitch (x-axis), and Yaw (y-axis)</figcaption>
                    </figure>
                </div>
                <p>I'll be referencing rotation in terms of <i>roll, pitch, and yaw</i> interchangeably with axes. Axes aren't universally consistent, but in this work, I use the <i>y-up</i> configuration
                    consistent with Unity, where <b>roll</b> is around the z-axis, <b>pitch</b> is around the x-axis, and <b>yaw</b> is around the y-axis.</p>
                <div class="row my-4">
                    <div class="col-sm-6">
                        <video autoplay muted loop style="max-width: 100%;">
                            <source type="video/mp4" src="video.mp4">
                        </video> 
                    </div>
                    <div class="col-sm-6">
                        <canvas id="chart"></canvas>
                    </div>
                </div>
                <p>These position values may also be referred to as <b>absolute position</b>, kept relative to an arbitrary static world origin. These values can encounter problems in
                    scenes with different origins or orientations. For example, if a linear regression were fit to a collection of scenes where the floor is at y-position $1.5$, and tested on a scene where the floor is at y-position
                    $-2$, it would likely fail.</p>
                <p>One way to address this is to use <b>relative position</b>, or position from the perspective of the viewer. In other words, no matter which way a viewer is
                    looking, an object moving directly away from the viewer will be moving along the <i>relative</i> z-axis. A caveat of this is that the axis of gravity is unstable,
                    since the direction of "up" changes with respect to the roll and pitch of the viewer.
                    Another option is what I'll refer to is <b>look position</b>, or position rotated about the y-axis with respect to the viewer. This way, $\{x, z\}$ rotate with
                    the direction the viewer is looking, but $y$ remains consistent with the global y-axis, or the axis of gravity. 
                    Both of these are sensitive to the motion of the viewer. That is, all objects in a scene moving upward look identical
                    to the viewer moving downward.</p>
                <p>Finally, I'll be referencing <b>velocity</b>, or the change in position over time. Velocity is robust to different scene origins and orientations, but is unable to capture
                    spatial relations, e.g. <i>the ball in on top of the table</i>.</p>
                <h4>Special Considerations</h4>
                <p>There are some things to consider when using euclidean spatial data, as opposed to words or images. These are not well-supported claims, just what I've noticed from my experiments.</p>
                <ul class="list-group">
                    <li class="list-group-item"><b>1. Transformers won't work.</b> The input is continuous and unbounded, so models that rely on discrete tokenized input, such as attention mechanisms, won't work.</li>
                    <li class="list-group-item"><b>2. There are no good labels.</b> Since the goal of this project is to learn representations of object motion that <i>may or may not align with verbs</i>,
                        there's no way to dictate exactly how a good representation should look. A good representation is one that is useful to carry out tasks higher in the abstraction tree, not necessarily
                        one that aligns perfectly with low-level English verbs.</li>
                    <li class="list-group-item"><b>3. It's difficult to normalize.</b> Euclidean space is infinite, so an apple may theoretically be moving around 7 billions meters away or 1 meter away.
                        This is in contrast to image data, where RGB values are bounded to $[0, 1]$. Typically, people just constrain the space and it isn't that big of a deal. But in trying to build
                        an intelligent agent that can adapt to various environments, it becomes a bigger problem.</li>
                    <li class="list-group-item"><b>4. Real spatial data is noisy.</b> Imagine someone picking up an apple. There may be a clear, consistent trajectory this action follows.
                        Now, compare this to a person quickly picking up an apple off a table in half a second. Compare that to someone picking up an apple off the ground in ten seconds, while shaking.
                        In reality, when looking at natural data of people <i>doing stuff</i>, good examples of "pick up" may have extremely different trajectories.</li>
                </ul>
            </div>
            <div id="models" class="mt-4">
                <h2>Models</h2>
                <h4>Preamble</h4>
                <p>Now that I've gone over spatial data and why I'm using it, I'll talk about the things I've actually tried so far. Each approach I've tried doesn't correspond 1:1 to a model, but to a many-to-many
                    collection of models, which makes it difficult to explain each approach in isolation. Therefore, I'll first go over the existing models I've used, before going over how the
                    pieces fit together in the <a href="#approaches">Approaches</a> section.</p>
                <p>However, before going into the models, I'll explain some technical parameters of the problem. I'm using the
                    <a href="https://lunar.cs.brown.edu/nbc/">New Brown Corpus</a> dataset, which consists of participants narrating while performing everyday tasks in virtual kitchens.</p>
                <div class="text-center my-4">
                    <video width="960" height="540" controls>
                        <source src="https://plunarlabcit.services.brown.edu/videos/1_1a_task1.mp4" type="video/mp4">
                    </video>
                </div>
                <p>The dataset is broken up into 108 <i>recording sessions</i> that are around 5 minutes long each. Each has corresponding narration, video, and spatial data. The spatial data includes
                    the <b>position</b> and <b>rotation</b> of every object (including the head and hands), every frame. There are 6 kitchens, 3 per participant. In all experiments, the participants are
                    split up 14-2-2 train-dev-test, so each kitchen is seen at least once at train time.</p>
                <p>The goal of each approach is to learn "actions", or cohesive, interesting patterns of motion that may align with verbs, or be useful for downstream verb-learning.
                    In general, learning representations of actions can be broken down into segmentation and clustering. <b>Segmentation</b> is cutting the sessions
                    into chunks of time, where each chunk of time corresponds to an action. <b>Clustering</b> is grouping similar chunks of time together, such that each cluster is representative of some
                    action. Finally, <b>Transformation</b> is manipulating the data to make it easier to do segmentation and clustering. Each model described serves one or more of these three purposes.</p>
                <!--TODO add timeline figure that shows segmentation pointing to graph that shows clustering-->
                <h4>Feature-engineering</h4>
                <p><i>Feature-engineering</i> is an umbrella term referring to any <b>transformations</b> I make that don't correspond to a model.
                    In practice, this typically consists of preprocessing steps like scaling or trimming outliers,
                    but may also involve more complex transformations like taking the magnitude velocity of the most moving object,
                    representing a trajectory in terms of its peak and trough, etc.</p>
                <h4>K-means</h4>
                <p>I use K-means for unsupervised <b>clustering</b>, which clusters data according to distance to centroids.
                    The number of clusters must be specified. Likewise for time-series data, I use 
                    <a href="https://tslearn.readthedocs.io/en/stable/gen_modules/clustering/tslearn.clustering.TimeSeriesKMeans.html">
                    Time Series K-Means</a>. With a
                    <a href="https://en.wikipedia.org/wiki/Dynamic_time_warping">Dynamic Time Warping</a> 
                    distance function, this algorithm allows time series data of different lengths to be clustered.</p>
                <p>Related to K-means, I also use <a href="https://www.is.mpg.de/uploads_file/attachment/attachment/442/1000-v2.pdf">Hierarchical Dynamic Clustering</a>,
                    a clustering algorithm for which the number of clusters is unspecified. Instead, a parameter is set that influences the number of clusters. This is
                    advantageous in cases where it's difficult to know a suitable number of clusters.</p>
                <h4>Energy-based motion pooling</h4>
                <p><a href="https://www.is.mpg.de/uploads_file/attachment/attachment/442/1000-v2.pdf">Energy-based motion pooling</a> 
                    is a <b>segmentation</b> model based on identifying energy or motion <i>peaks</i>, and considering windows around them to
                    be motion <i>segments</i>. The way this is done is as follows:</p>
                <ol>
                    <li>Discretize the position data into many small clusters, effectively binning the data.</li>
                    <li>Using a sliding window across the data, compute the <i>energy</i> of each window, or number of transitions between clusters.</li>
                    <li>Apply a gaussian smoothing filter to the energy curve.</li>
                    <li>Identify peaks in the energy curve.</li>
                    <li>Designate actions as windows around peaks based on peak prominence.</li>
                </ol>
                <h4>Hidden semi-markov model</h4>
                <p>The <a href="https://en.wikipedia.org/wiki/Hidden_semi-Markov_model">hidden semi-markov model</a> is a bayesian model that simultaneously performs <b>segmentation</b> and <b>clustering</b>.
                    Like a hidden markov model, it consists of states and transitions, and the ability to compute the maximum
                    likelihood state sequence given some data. However, unlike a hidden markov model,
                    the hidden <b>semi-</b>markov model also considers the number of timesteps or expected <i>length</i> in each state.
                    By taking the maximum-likelihood state sequence of a data sequence given a hidden semi-markov model, we
                    end up with a segmented, labeled sequence.</p>
                <h4>Principle Component Analysis</h4>
                <p>Principle Component Analysis or PCA is a common algorithm that reduces the dimensionality of data while trying
                    to retain as much information as possible. It is a popular <b>transformation</b> of data.</p>
                <h4>Autoencoder</h4>
                <p>Like PCA, an autoencoder or AE <b>transforms</b> data by encoding it to a smaller representation.
                    It does so using an encoder-decoder neural network architecture, where the encoder network encodes
                    the data to a reduced representation, and the decoder network attempts to reconstruct the original input
                    from the reduced representation.</p>
                <p>I also use a variational autoencoder or VAE, a probabilistic variation of the autoencoder that produces a smooth
                    encoded manifold. This means that, if unseen points are sampled from the encoding space, it should
                    produce plausible outputs.</p>
                <h4>LSTM</h4>
                <p>Finally, I <b>transform</b> data using an LSTM, a form of recurrent neural network that models sequential data
                    by processing timesteps in terms of previous timesteps. LSTMs are very popular modern baseline when working
                    with sequential data like sentences or videos.</p>
            </div>
            <div id="approaches">
                <h2>Approaches</h2>
                <h4>HSMM based</h4>
                <p>Most of my time spent working with spatial data, I've done so with an HSMM based pipeline.
                    This pipeline can be summarized as follows:</p>
                <ol>
                    <li>Some <b>transformations</b> to the data, such as <b>feature-engineering</b>, <b>autoencoders</b> or
                    <b>LSTMs</b>.</li>
                    <li>An <b>HSMM</b> for simultaenous <b>segmentation</b> and <b>clustering</b>.</li>
                </ol>
                <p>Throughout these attempts, representing months of research, the question was:</p>
                <blockquote class="blockquote px-5">How can I transform the data so that the HSMM is able to produce the best
                    possible segments and clusters?</blockquote>
                <p>For the HSMM itself, I stuck to a Gaussian observation model. That is, each state can only produce
                    values according to some mean and variation. This means that a state is unable to represent dynamic data,
                    i.e. a single state can't represent the $\{x, y, z\}$ position of an object moving from one place to another.
                    Therefore, the goal is to represent this motion in terms of values that don't change (except when trying to
                    represent a different motion), before feeding it to the hsmm.</p>
                <p>One limitation of HSMM is memory complexity, at least for the torch-struct library I used. I could only 
                    handle up to 8 states at once, which may have been a problematic constraint.</p>
                <h5>Evaluation</h5>
                <p>For all approaches I'll discuss below, I evaluated with an odd-man-out evaluation paradigm. That is,
                    given some labeled segments, I select 3 corresponding video clips with the same label, and 1 with a
                    different label. Then, I shuffle and present these and ask: which one is the odd man out? The idea is that,
                    if the clusters are good, it should be easy to identify which one doesn't belong.
                <p><b>All approaches I'll discuss, unless specified otherwise, performed at random on the odd-man-out
                evaluation.</b> So, for each, I'll talk about where it succeeded <i>qualitatively</i>, and
                why or how it failed at odd-man-out, but I won't restate that it performed at random each time.</p>
                <h5>Direct Inputs</h5>
                <p>The simplest way to represent motion as a single value is to take the derivative. The derivative of position is
                    <b>velocity</b>. I tried directly inputting velocity at each timestep to the HSMM with the following configurations:</p>
                <ul class="list-group mb-3" id="velocity-trials">
                    <li class="list-group-item">Right hand relative $z$, or movement of the right hand to/from the head.
                        This successfully captures some recognizable away-from-head or toward-head segments, but is
                        muddled by the subtle, noisy motions in the data.</li>
                    <li class="list-group-item">Right hand relative $\{y, z\}$, or height and depth motion of the right hand.
                        This performs similarly to only depth and doesn't cause noticable qualitative change.</li>
                    <li class="list-group-item">Both hands relative $z$ and $\{y, z\}$. There are still some recognizable motions,
                        now with both hands, but overall appears more muddled than before.</li>
                    <li class="list-group-item">Relative $\{x, y, z\}$ velocity of the most-moving object. This successfully captures some
                        notion of when an object is moving, but is indifferentiable otherwise.</li>
                    <li class="list-group-item">Right hand <i>and</i> most-moving object relative $\{x, y, z\}$. Some reconizeable motions,
                        but not clear.</li>
                    <li class="list-group-item">Right hand $\{x, y, z\}$ and most-moving object <i>magnitude</i> velocity. A bit clearer than
                        before, but still, of course, unsuccessful in odd-man-out.</li>
                    <li class="list-group-item">Right hand $\{x, y, z\}$ and binary value for whether any object is moving. Yet a bit clearer
                        than before, allowing the model to easily distinguish object-moving hand motions from other hand motions, but still
                        unsuccessful in odd-man-out, and feels a bit hacky.</li>
                </ul>
                <p>Given lack of success of odd-man-out on any of these attempts, I did also try a sanity-check setup strongly expected to succeed.
                    That is, <i>is any object moving?</i> For this, I transform the data such that it's only a binary value at each timestep
                    for whether any object velocity is above some threshold. I train a 2-class HSMM. This does reach an odd-man-out accuracy
                    of 90%. Even this is surprisingly low given how simple the task seems. Qualitatively, it can sometimes be difficult to tell
                    whether an object is even moving in a video, since many clips consist of a participant hovering their hand over an object
                    and barely moving it, to such a fine degree that it's difficult to distinguish if it should actually be considered moving.
                    This is something that I think really helps, when evaluting, to illuminate the challenge of the dataset.</p>
                </p>
                <h5>Autoencoder</h5>
                <p>I thought an autoencoded representation of motion may be more robust than direct velocity. So, using a sliding
                    window over the data where window size $w=\{.5s, 1s, 2s\}$, I divide the data into chunks that I encode with
                    a VAE. I do a search of many different VAE hyperparameters and build an interface to thoroughly debug what the
                    autoencoder is doing, representing some weeks of work. Some particular configurations are as follows:</p>
                <ul class="list-group mb-3">
                    <li class="list-group-item">Autoencoders with velocity as input, similar to in <a href="#velocity-trials">Direct Inputs</a>
                        These performed similarly to direct inputs.</li>
                    <li class="list-group-item">Concatenated autoencoder representations, where a different autoencoder is trained for
                        each hand, each object, etc., then concatenated. This didn't work at all, which makes sense, since
                        the latent spaces are completely unrelated.</li>
                    <li class="list-group-item">Hand and object autoencoders, where a separate autoencoder is trained on hand motion, and
                        another on object motion. The most-moving-hand and most-moving-object encoding are concatenated. This provides some
                        identifiable motions, and seems qualitatively to be the best of the autoencoder > HSMM approaches, despite still
                        failing the odd-man-out evaluation.</li>
                    <li class="list-group-item">Same as previous, concatenated with a one-hot vector indicating which object is moving.
                        This should theoretically separate object motions by object identity, but still suffers the same problems as before,
                        and is hindered by the memory constraints of the HSMM to have enough states to separately represent motion for each object.</li>
                </ul>
                <p>I also tested a vanilla autoencoder and disentangled VAE, but neither of these yielded better performance. I also tried different
                    preprocessing steps, including clipping the velocity to a limited range, z-norm, min-max scaling, and robust scaling.</p>
                <h5>LSTM</h5>
                <p>I tested an LSTM > HSMM approach similar to the VAE > HSMM approach above. However, instead of transforming the data with a VAE,
                    I do so with an LSTM, by taking the final hidden state of an LSTM trained to predict the next timestep of a sequence. To avoid
                    making the task too easy, I experiment with adding a delay to the prediction, i.e. the LSTM should predict velocity half a second after
                    the input sequence.</p>
                <p>This was never successful, always providing qualitatively indecipherable representations. To try to rule out bugs,
                    I visualized the success of the delayed prediction model, which is qualitatively nearly perfect without a delay, very good with a half-second delay,
                    and okay with a one-second delay. I also set it up as a classification task, using a small dataset of expert-labeled reach,
                    pick, put, and retract actions, which it succeeds at. So, it seems the LSTM works, but for another reason doesn't provide
                    decipherable representations.</p>
                <h4>K-means based</h4>
                <p>More recently, I opted for an HSMM-free approach, for which segmentation is separate from clustering, and K-means
                    is used for clustering. For this approach, I only used one pipeline, as follows:</p>
                <ol>
                    <li>Use Hierarchical Dynamic Clustering to discretize absolute position data.</li>
                    <li>Apply Energy-based Motion Pooling for segmentation.</li>
                    <li>Cluster segments using Time Series K-means clustering.</li>
                </ol>
                <p>Using Energy-based Motion Pooling for segmentation was qualitatively very successful - the segments appear to
                    correspond very well to clearly identifiable actions of varying granularity. For example, moving an object
                    from one place to another may be a segment, and within that segment, a pick motion may be another segment.
                    This is because these each correspond to motion peaks, but the prominence of one peak is much higher,
                    causing its surrounding window to encompass the other's.</p>
                <p>Despite this good looking segmentation, clustering wasn't so successful. I wasn't able to qualitatively
                    evaluate the clusters, since the interface wasn't set up to handle overlapping clusters (which may be a 
                    future to-do). But in the odd-man-out evaluation, no clusters were identifiable at all, yielding random accuracy.</p>
                <p>It's worth noting that, since segmentation and clustering are separate here, any number of clustering models could be
                    applied - I just only tested K-means. I can imagine a number of possible next steps here, but given that this 
                    "almost there, just need to try one more thing" phenomenon has been going on for over a year, I decided to instead
                    write up this post giving an overview of everything attempted so far.</p>
            </div>
            <div id="conclusion">
                <h2>Takeaways</h2>
                <p>My personal takeaway is that, many of these models seem to be <i>almost there</i>, but none have been successful.
                    I think that the primary issue is the data - it's very small and noisy. Something that I think really highlights
                    this is the fact that the <i>is any object moving?</i> setup only gets 90% accuracy. Natural motion is just
                    really hard to clearly divide into actions, using spatial data alone. Some options I see are:</p>
                <ol>
                    <li>Abandon spatial data, move on to more abstract inputs, like seen in ALFRED.</li>
                    <li>Keep testing more models and try to wrap up this chapter on spatial data.</li>
                    <li>Go all-in on spatial data and develop a larger, artificial dataset for developing physics-based 
                        grounding models. A sufficiently large dataset in a constrained environment may make
                        transformer-based models feasible.</li>
                </ol>
            </div>
        </div>
        <footer class="py-5 mt-5 border-top"></footer>
    </body>
    <script>
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });

        $.get('https://raw.githubusercontent.com/dylanebert/spatial_blog/master/data.json', function(res) {
            var data = $.parseJSON(res);
            console.log(data);
            var ctx = $('#chart');
            var chart = new Chart(ctx, {
                type: 'line',
                data: data
            })
        })
    </script>
</html>