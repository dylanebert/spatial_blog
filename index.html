<html>
    <head>
        <link href='https://fonts.googleapis.com/css?family=Roboto' rel='stylesheet'>
        <script src="https://code.jquery.com/jquery-3.6.0.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" integrity="sha384-B0vP5xmATw1+K9KRQjQERJvTumQW0nPEzvF6L/Z6nronJ3oUOFUFpCjEUQouq2+l" crossorigin="anonymous">
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" integrity="sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.9.2/umd/popper.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    </head>
    <style>
        body {
            font-family: 'Roboto';
        }
        h2 {
            padding: 10px 0;
            font-weight: bold;
        }
    </style>
    <body>
        <div class="container">
            <h1 class="py-2 mt-5 border-bottom font-weight-bold">Adventures in Grounding to Object Positions</h1>
            <a class="text-muted" href="https://dylanebert.com">Dylan Ebert</a>
            <div class="mt-3" id="overview">
                <p>This document gives an overview of my recent attempts to make use of spatial data,
                    or the positions of objects over time, to represent verbs.
                    I'll talk about the motivation for this, the models I've tried, and the results so far.
                </p>
            </div>
            <div id="motivation">
                <h2>Why spatial data?</h2>
                <h4>Text-based NLP</h4>
                <p>Most recent progress in NLP takes advantage of very large-scale language datasets and models to solve text-based tasks
                    like question-answering. Take a look at the BoolQ task below.</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="qbool.PNG" class="img-fluid">
                        <figcaption class="figure-caption">BoolQ task from SuperGLUE, a common NLP benchmark.</figcaption>
                    </figure>
                </div>
                <p>The input is text, and the output is text. This is the common setup for a question-answering task.
                    <a href="https://arxiv.org/pdf/2103.08493.pdf">Prompt-based training</a> sets up a variety of tasks
                    like a question-answering tasks, and achieves human performance on many of them in a unified framework.
                    So, is language solved? No, not yet. So, what's missing? I, among many others, would argue that grounding is a necessary
                    component of language understanding.</p>
                <h4>Grounded NLP</h4>
                <p>Grounding is the connection of language to the non-linguistic world. What does recent work in grounded NLP look like? 
                    Some years ago, grounding meant image captioning, i.e. is below a picture of a zebra?</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Plains_Zebra_Equus_quagga.jpg/399px-Plains_Zebra_Equus_quagga.jpg" class="img-fluid">
                        <figcaption class="figure-caption">Zebra?</figcaption>
                    </figure>
                </div>
                <p>More recent work has <a href="https://arxiv.org/pdf/2102.02779.pdf">unified vision and language models</a> to learn representations that incorporate both images and text.
                    This still falls short of human performance on tasks such as visual question-answering, but moves the needle toward it.
                    So, with bigger models, more data, and better performance, will language be solved? I don't know. It's more difficult to support an answer to that than to whether grounding is necessary.
                    However, it does raise another interesting question - what else can we do?</p>
                <h4>Embodied NLP</h4>
                <p>So far, I've only talked about tasks that produce some text output. But what about tasks that require doing something in the world?
                    One example is <a href="https://askforalfred.com/">ALFRED</a>, a benchmark for mapping natural language instructions to sequences of actions.
                    In ALFRED, a virtual agent takes egocentric video and text instructions as input, and must execute <i>actions</i> to carry out the instructions.</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="alfreddiagram.png" class="img-fluid">
                        <figcaption class="figure-caption">Example instruction sequence from ALFRED</figcaption>
                    </figure>
                </div>
                <p>This task involves the complex behaviors of objects, which makes for a much more challenging task. This is reflected in the current state-of-the-art performance,
                    which is <a href="https://leaderboard.allenai.org/alfred/submissions/public">under 10% for unseen sequences</a>.
                    Why is the performance so low? What makes this task so much more challenging than text-based tasks? These are the questions I'm most interested in.</p>
                <h4>Broader Perspectives: Abstraction</h4>
                <p>Taking a look at artificial intelligence as a whole, where do text-based, grounded, and embodied NLP fit? 
                    I'll borrow from <a href="https://www.sciencedirect.com/science/article/pii/S2352154618302080">On the Necessity of Abstraction</a>,
                    which highlights the importance of <i>abstraction</i>, or mapping a complex state space to a more simple state space that is computationally tractable to carry out a given task.
                    For example, imagine a game of chess. At the lowest layer of abstraction, we have raw sensory information - visual, auditory, and tactile. At a high level of abstraction,
                    we have an 8x8 grid of chess pieces. But when people think of chess, and when building AI to solve chess, it's not in terms of matrices of RGB values, 
                    but rather the 8x8 grid. Although, sensory input is necessary for actually playing chess, in order to recognize pieces, situate oneself in the environment to
                    reach the pieces, and engage muscles to execute a move.</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="obj-recog.png" class="img-fluid p-3" style="max-width: 512px;">
                        <figcaption class="figure-caption">Abstraction tree for object recognition</figcaption>
                    </figure>
                </div>
                <p>Object recognition, a classic computer vision problem, is a clear example of abstraction. The inputs are images composed of a matrices of RGB values, and the output is object labels.
                    A caveat of this is that the object labels themselves correspond to language, which is entangled with many different levels of abstraction. However, these labels represent
                    objects, or persitent entities that are expected to maintain shape and move continuously. This is an abstraction on raw visual input that is shown to exist from infancy, long before language.</p>
                <p>So, what might language look like in an abstraction tree?</p>
                <div class="text-center">
                    <figure class="figure">
                        <img src="grounded.png" class="img-fluid p-3" style="max-width: 512px;">
                        <figcaption class="figure-caption">Abstraction tree for language</figcaption>
                    </figure>
                </div>
                <p>Above is a simplified example of how language may fit into an abstraction tree, in terms of problems in AI. Language, in the top right, is composed only of nouns and verbs, for simplicity.
                    In most modern language models, this is a self-contained subtree, where language is based only on language. In the case of grounded language models, additional information comes directly from vision,
                    at the bottom of the abstraction tree. Elsewhere in the tree are fields like speech recognition, which abstracts from audio to words, and simulation, which constrains the objects to some common behavior,
                    labeled here as "physics". These don't play any direct role in state-of-the-art language models. So how are these models successful?</p>
                <h4>Fitting things together</h4>
                <p>Though modern language models don't explicitly use information from lower-level inputs like physics or audio, probing experiments have shown that language models contain information
                    that is applicable to a wide variety of tasks. This may be explained by the idea that large amounts of information about the world and our perception at various levels of abstraction are baked into
                    our use of language, and can be learned through distributional language models. However, how and the extent to which this information is learned is unclear, and, as previously stated, aren't succeeding
                    at complex situated tasks like instruction following.</p>
                <p>The purpose of this research is to hone in on a sparsely-researched part of the abstraction tree: verbs from physics, as shown in the figure above. Most prior work in verb learning uses vision as input. Learning verbs from
                    video is a very challenging problem. Humans, on the other hand, already have some mental model of objects and their behavior as infants, and acquire language later. Given this information, the questions
                    this research asks are:</p>
                <p></p>
                <ol class="list-group">
                    <li class="list-group-item">Using the spatial data of objects as input, can we learn representations of verbs? And if not, what can we represent?</li>
                    <li class="list-group-item">What is the difference between what is learned from object-based models compared to vision-based models?</li>
                </ol>
            </div>
            <div id="spatial" class="mt-5">
                <h2>Spatial data: In practice</h2>
                <p>Though we know that humans are able to represent objects and their physical behavior from infancy, what does this representation actually look like? Unfortunately, we don't know.
                    In my case, I choose to go with common practice - euclidean space, where each object position is represented as a 3d vector with components $\{x, y, z\}$.
                </p>
                <div class="row">
                    <div class="col-sm-6">

                    </div>
                    <div class="col-sm-6">
                        <canvas id="chart"></canvas>
                    </div>
                </div>
            </div>
        </div>
        <footer class="py-5 mt-5 border-top"></footer>
    </body>
    <script>
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });

        var ctx = $('#chart');
        var chart = new Chart(ctx, {
            type: 'line',
            data: {
                labels: [100, 200],
                datasets: [{
                    data: [10, 20],
                    label: 'test'
                }]
            }
        })
    </script>
</html>